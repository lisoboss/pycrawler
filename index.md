# Python爬虫


## Request中包含什么呢？

1. 请求方式：主要有GET和POST两种方式,POST请求的参数不会包含在url里面 
2. 请求URL `URL：统一资源定位符，如一个网页文档、一张图片、一个视频等都可以用URL来唯一确定`
3. 请求头信息,包含了User-Agent（浏览器请求头）、Host、Cookies信息
4. 请求体,GET请求时，一般不会有，POST请求时，请求体一般包含form-data


## Response中包含什么信息？

1. 响应状态：状态码 正常响应200 重定向
2. 响应头：如内容类型、内容长度、服务器信息、设置cookie等
3. 响应体信息：响应源代码、图片二进制数据等等


## 常见的http状态码

- 200状态码 服务器请求正常 301状态码：被请求的资源已永久移动到新位置。服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置。 
- 302状态码：请求的资源临时从不同的URI响应请求，但请求者应继续使用原有位置来进行以后的请求
- 401状态码：请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。
- 403状态码：服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。
- 404状态码：请求失败，请求所希望得到的资源未被在服务器上发现。
- 500状态码：服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器的程序码出错时出现。
- 503状态码：由于临时的服务器维护或者过载，服务器当前无法处理请求。


## HTTP 的请求和响应都包含哪些内容

### HTTP请求头 

- Accept:浏览器能够处理的内容类型 
- Accept-Charset:浏览器能够显示的字符集
- Accept-Encoding：浏览器能够处理的压缩编码 
- Accept-Language：浏览器当前设置的语言
- Connection：浏览器与服务器之间连接的类型 
- Cookie：当前页面设置的任何Cookie 
- Host：发出请求的页面所在的域
- Referer：发出请求的页面的URL 
- User-Agent：浏览器的用户代理字符串
 
### HTTP响应头部信息：

- Date：表示消息发送的时间，时间的描述格式由rfc822定义 
- server:服务器名字。
- Connection：浏览器与服务器之间连接的类型 
- Content-type:表示后面的文档属于什么MIME类型
- Cache-Control：控制HTTP缓存


## mysql的索引在什么情况下失效

1. 如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因) 要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
2. 对于多列索引，不是使用的第一部分，则不会使用索引
3. like查询以%开头
4. 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引
5. 如果mysql估计使用全表扫描要比使用索引快,则不使用索引


## MySQL 有什么引擎，各引擎之间有什么区别？

主要 MyISAM 与 InnoDB 两个引擎，其主要区别如下：

1. InnoDB 支持事务，MyISAM不支持，这一点是非常之重要。事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而 MyISAM就不可以了；
2. MyISAM 适合查询以及插入为主的应用，InnoDB 适合频繁修改以及涉及到安全性较高的应用； 
3. InnoDB支持外键，MyISAM 不支持； 
4. MyISAM 是默认引擎，InnoDB 需要指定； 
5. InnoDB 不支持 FULLTEXT类型的索引； 
6. InnoDB 中不保存表的行数，如 select count() from table 时，InnoDB；需要扫描一遍整个表来计算有多少行，但是 MyISAM 只要简单的读出保存好的行数即可。注意的是，当 count()语句包含 where 条件时 MyISAM 也需要扫描整个表； 
7. 对于自增长的字段，InnoDB 中必须包含只有该字段的索引，但是在 MyISAM表中可以和其他字段一起建立联合索引； 
8. 清空整个表时，InnoDB 是一行一行的删除，效率非常慢。MyISAM 则会重建表；
9. InnoDB 支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like ‘%lee%’


## Scrapy优缺点：

### 优点

- scrapy 是异步的
- 采取可读性更强的xpath代替正则
- 强大的统计和log系统
- 同时在不同的url上爬行
- 支持shell方式，方便独立调试
- 写middleware,方便写一些统一的过滤器
- 通过管道的方式存入数据库

### 缺点

- 基于python的爬虫框架，扩展性比较差
- 基于twisted框架，运行中的exception是不会干掉reactor，并且异步框架出错后是不会停掉其他任务的，数据出错后难以察觉。


## HTTPS 是如何实现安全传输数据的

1. 客户端（通常是浏览器）先向服务器发出加密通信的请求
2. 服务器收到请求,然后响应
3. 客户端收到证书之后会首先会进行验证
4. 服务器收到使用公钥加密的内容，在服务器端使用私钥解密之后获得随机数pre-master secret，然后根据radom1、radom2、pre-master secret通过一定的算法得出session Key和MAC算法秘钥，作为后面交互过程中使用对称秘钥。同时客户端也会使用radom1、radom2、pre-master secret，和同样的算法生成session Key和MAC算法的秘钥。
然后再后续的交互中就使用session Key和MAC算法的秘钥对传输的内容进行加密和解密。


## 描述下scrapy 框架运行的机制

> 从start_urls里获取第一批url并发送请求，请求由引擎交给调度器入请求队列，获取完毕后，调度器将请求队列里的请求交给下载器去获取请求对应的响应资源，并将响应交给自己编写的解析方法做提取处理：
> 如果提取出需要的数据，则交给管道文件处理；
> 如果提取出url，则继续执行之前的步骤（发送url请求，并由引擎将请求交给调度器入队列…)，直到请求队列里没有请求，程序结束。
